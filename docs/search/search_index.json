{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview The purpose of this project (nicknamed \"PyRunner\") is to provide a lightweight, extensible, and non-opinionated development framework for batch applications. It is intended to simply provide the scaffolding for a batch application without the weight of an all-inclusive or centralized platform, such that the resulting product is portable and not tied down to a specific scheduling or monitoring solution. The framework encourages deconstructing the steps of a process down to atomic units of logic, such that application state/progress can be managed as a collection of tasks. It provides the following with zero or minimal setup required: Cross-task data/information sharing Parallel execution and configurability of task order/dependencies Log management Job notifications Self-managed restartability from point of failure Job and task level lifecycle hooks for optional code execution at various steps (such as in the event of failure or job restart) Option to implement custom loggers, notifications, workers, etc. Installation pip install python-batch-runner Usage New Project Setup A simple setup function is included and can be run by executing: pyrunner --setup This will prompt you for three inputs: Project Name Provide a name for your project/application without any spaces (i.e. MySampleProject). If spaces are included, they will be removed. Project Path Provide the path to create the project directory in. The project name (lowercased) will be appended to this path. If left blank, the path will default to the current working directory. Upon completion, a new directory at the provided (or default/current) path will be created, along with minimum necessary subdirectory and files. Please see the Basic Project Structure page for more details. How to Execute The most basic PyRunner command requires only the app_profile and .lst file : # Basic execution with only app_profile and .lst file pyrunner -c <app_root_path>/config/app_profile -l <app_root_path>/config/<project_name>.lst # If args needed pyrunner -c <app_root_path>/config/app_profile -l <app_root_path>/config/<project_name>.lst -r --env PROGRAM_NAME=MY_SAMPLE_PROGRAM This will instruct PyRunner to source the app_profile (this is required to provide PyRunner with required application details, such as log and temp dir path) and then execute the workflow described in the .lst file. The preferred method, however, is to use a driver Python program, which unlocks access to the framework's full capabilities. For your convenience, if the --setup utility was used, the <app_name>.py ( my_app.py in this example) file is created which contains: #!/usr/bin/env python3 import os, sys from pyrunner import PyRunner from pathlib import Path # Determine absolute path of this file's parent directory at runtime abs_dir_path = os.path.dirname(os.path.realpath(__file__)) # Store path to default config and .lst file config_file = '{}/config/app_profile'.format(abs_dir_path) proc_file = '{}/config/my_app.lst'.format(abs_dir_path) # Init PyRunner and assign default config and .lst file app = PyRunner(config_file=config_file, proc_file=proc_file) if __name__ == '__main__': # Initiate job and exit driver with return code sys.exit(app.execute()) This can then simply be executed as below. Note that this method of execution has access to ALL of the same arguments as the pyrunner -c ... form. # Basic execution with only app_profile and .lst file ./my_app.py # If args needed, then simply pass as arguments ./my_app.py -r --env PROGRAM_NAME=MY_SAMPLE_PROGRAM # We can even use the -c and -l options to override the config/lst file # specified in the driver. ./my_app.py -c /path/to/other/app_profile -l /path/to/other/alternate.lst Execution Options Option Argument Description --env [variable_name]=[variable_value] Set environment variable - equivalent to export [variable_name]=[variable_value] --cvar [variable_name]=[variable_value] Set context variable to be available at the start of job. -r Restart flag. Causes PyRunner to check the APP_TEMP_DIR for existing .ctllog files to restart a job from failure. Fresh run if no .ctllog file found. -n or --max-procs integer Sets the absolute maximum number of parallel processes allowed to run concurrently. -x or --exec-only comma separated list of process ID's Executes only the given process ID(s) from the .lst file. --exec-proc-name single process name Similar to --exec-only - Executes only the process ID identified by the given process name. -A or --to or --ancestors single process ID Executes given process ID and all preceding/ancestor processes. -D or --from or --descendents single process ID Executes given process ID and all subsequent/descendent processes. -N or --norun comma separated list of process ID's Prevents the given process ID(s) from executing. -e or --email email address Sets email address to send job notification email after run completion. Overrides all other APP_EMAIL settings. --es or --email-on-success true/false or 1/0 Enables or disables email notifications when job exits with success. Default is True. --ef or --email-on-fail true/false or 1/0 Enables or disables email notifications when job exits with failure. Default is True. -i or --interactive Primarily for use with -x option. Launches in interactive mode which will request input from user if a Context variable is not found. -d or --debug Debug option that only serves to provide a more detailed output during execution to show names of pending, running, failed, etc. tasks. --dump-logs Enables job to dump to STDOUT logs for all failed tasks after job exits. --nozip Disables zipping of log files after job exits. -t or --tickrate Sets the number of checks per second that the execution engine performs to poll running processes. -h or --help Prints out options and other details. -v or --version Prints out the installed PyRunner version.","title":"Home"},{"location":"#overview","text":"The purpose of this project (nicknamed \"PyRunner\") is to provide a lightweight, extensible, and non-opinionated development framework for batch applications. It is intended to simply provide the scaffolding for a batch application without the weight of an all-inclusive or centralized platform, such that the resulting product is portable and not tied down to a specific scheduling or monitoring solution. The framework encourages deconstructing the steps of a process down to atomic units of logic, such that application state/progress can be managed as a collection of tasks. It provides the following with zero or minimal setup required: Cross-task data/information sharing Parallel execution and configurability of task order/dependencies Log management Job notifications Self-managed restartability from point of failure Job and task level lifecycle hooks for optional code execution at various steps (such as in the event of failure or job restart) Option to implement custom loggers, notifications, workers, etc.","title":"Overview"},{"location":"#installation","text":"pip install python-batch-runner","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#new-project-setup","text":"A simple setup function is included and can be run by executing: pyrunner --setup This will prompt you for three inputs: Project Name Provide a name for your project/application without any spaces (i.e. MySampleProject). If spaces are included, they will be removed. Project Path Provide the path to create the project directory in. The project name (lowercased) will be appended to this path. If left blank, the path will default to the current working directory. Upon completion, a new directory at the provided (or default/current) path will be created, along with minimum necessary subdirectory and files. Please see the Basic Project Structure page for more details.","title":"New Project Setup"},{"location":"#how-to-execute","text":"The most basic PyRunner command requires only the app_profile and .lst file : # Basic execution with only app_profile and .lst file pyrunner -c <app_root_path>/config/app_profile -l <app_root_path>/config/<project_name>.lst # If args needed pyrunner -c <app_root_path>/config/app_profile -l <app_root_path>/config/<project_name>.lst -r --env PROGRAM_NAME=MY_SAMPLE_PROGRAM This will instruct PyRunner to source the app_profile (this is required to provide PyRunner with required application details, such as log and temp dir path) and then execute the workflow described in the .lst file. The preferred method, however, is to use a driver Python program, which unlocks access to the framework's full capabilities. For your convenience, if the --setup utility was used, the <app_name>.py ( my_app.py in this example) file is created which contains: #!/usr/bin/env python3 import os, sys from pyrunner import PyRunner from pathlib import Path # Determine absolute path of this file's parent directory at runtime abs_dir_path = os.path.dirname(os.path.realpath(__file__)) # Store path to default config and .lst file config_file = '{}/config/app_profile'.format(abs_dir_path) proc_file = '{}/config/my_app.lst'.format(abs_dir_path) # Init PyRunner and assign default config and .lst file app = PyRunner(config_file=config_file, proc_file=proc_file) if __name__ == '__main__': # Initiate job and exit driver with return code sys.exit(app.execute()) This can then simply be executed as below. Note that this method of execution has access to ALL of the same arguments as the pyrunner -c ... form. # Basic execution with only app_profile and .lst file ./my_app.py # If args needed, then simply pass as arguments ./my_app.py -r --env PROGRAM_NAME=MY_SAMPLE_PROGRAM # We can even use the -c and -l options to override the config/lst file # specified in the driver. ./my_app.py -c /path/to/other/app_profile -l /path/to/other/alternate.lst","title":"How to Execute"},{"location":"#execution-options","text":"Option Argument Description --env [variable_name]=[variable_value] Set environment variable - equivalent to export [variable_name]=[variable_value] --cvar [variable_name]=[variable_value] Set context variable to be available at the start of job. -r Restart flag. Causes PyRunner to check the APP_TEMP_DIR for existing .ctllog files to restart a job from failure. Fresh run if no .ctllog file found. -n or --max-procs integer Sets the absolute maximum number of parallel processes allowed to run concurrently. -x or --exec-only comma separated list of process ID's Executes only the given process ID(s) from the .lst file. --exec-proc-name single process name Similar to --exec-only - Executes only the process ID identified by the given process name. -A or --to or --ancestors single process ID Executes given process ID and all preceding/ancestor processes. -D or --from or --descendents single process ID Executes given process ID and all subsequent/descendent processes. -N or --norun comma separated list of process ID's Prevents the given process ID(s) from executing. -e or --email email address Sets email address to send job notification email after run completion. Overrides all other APP_EMAIL settings. --es or --email-on-success true/false or 1/0 Enables or disables email notifications when job exits with success. Default is True. --ef or --email-on-fail true/false or 1/0 Enables or disables email notifications when job exits with failure. Default is True. -i or --interactive Primarily for use with -x option. Launches in interactive mode which will request input from user if a Context variable is not found. -d or --debug Debug option that only serves to provide a more detailed output during execution to show names of pending, running, failed, etc. tasks. --dump-logs Enables job to dump to STDOUT logs for all failed tasks after job exits. --nozip Disables zipping of log files after job exits. -t or --tickrate Sets the number of checks per second that the execution engine performs to poll running processes. -h or --help Prints out options and other details. -v or --version Prints out the installed PyRunner version.","title":"Execution Options"},{"location":"example/","text":"Traditionally, batch processes/applications are commonly built as top-down scripts. It is not uncommon to see even long-running batch processes implemented as long top-down Shell/Python scripts or a series of individual scripts to be executed in specific order. In the event of a failure, processes may need to be resumed/restarted from the specific point of failure, rather than from the start. To do this, state management must either be implemented or a larger orchestration tool may be used. Sample Python Script Let's take a very simple (and very much contrived) example: import urllib.request import pandas as pd from datetime import datetime def emit_slack_message(message): # Assume implementation # ... def load_to_mysql(df): # Assume implementation # ... def write_as_parquet(df, path): # Assume implementation # ... # Broadcast job start notification print('Starting example batch process') run_date_str = datetime.now().strftime('%Y-%m-%d') emit_slack_message('Starting daily data download for {}'.format(run_date_str)) # Download data file 1 print('Initiating File Download 1') url1 = 'https://some_file_url_here_1' urllib.request.urlretrieve(url1, '/landing/data1.csv') # Download data file 2 print('Initiating File Download 2') url2 = 'https://some_file_url_here_2' urllib.request.urlretrieve(url2, '/landing/data2.csv') # Load file 1 to MYSQL print('Loading MYSQL table') df1 = pd.read_csv('/landing/data1.csv') load_to_mysql(df1) # Write combined file as parquet print('Writing Parquet file') df2 = pd.read_csv('/landing/data2.csv') write_as_parquet(df2, '/data/extract_{}.parquet'.format(run_date_str)) # Broadcast job end notification print('Completed example batch process') emit_slack_message('Successfully completed daily data download for {}'.format(run_date_str)) As with any script, all steps execute in serial fashion and upon failure, a restart will run all steps again. Worker Classes We can instead convert this into a PyRunner application with minor changes by separating each logical step into Worker classes: # <app_root_dir>/python/workers.py import urllib.request import pandas as pd from datetime import datetime from pyrunner import Worker def emit_slack_message(message): # Assume implementation # ... def load_to_mysql(df): # Assume implementation # ... def write_as_parquet(df, path): # Assume implementation # ... # Broadcast job start notification class Start(Worker): def run(self): # Print function also works, but we can take advantage of # advanced features with the provided logger. self.logger.info('Starting example batch process') run_date_str = datetime.now().strftime('%Y-%m-%d') emit_slack_message('Starting daily data download for {}'.format(run_date_str)) # The self.context is a special thread-safe shared dictionary, # which can be read or modified from any Worker. self.context['run_date_str'] = run_date_str # Download data file 1 class DownloadFile1(Worker): def run(self): self.logger.info('Initiating File Download 1') url = 'https://some_file_url_here_1' urllib.request.urlretrieve(url, '/landing/data1.csv') # Download data file 2 class DownloadFile2(Worker): def run(self): self.logger.info('Initiating File Download 2') url = 'https://some_file_url_here_2' urllib.request.urlretrieve(url, '/landing/data2.csv') # Load file 1 to MYSQL class LoadMySQL(Worker): def run(self): self.logger.info('Loading MySQL table') df = pd.read_csv('/landing/data1.csv') load_to_mysql(df) # Write file 2 as parquet class WriteParquet(Worker): def run(self): self.logger.info('Writing Parquet file') # Once again, we are accessing the shared dictionary, this time # to read the value originally set by the Start Worker. run_date_str = self.context['run_date_str'] df = pd.read_csv('/landing/data2.csv') write_as_parquet(df, '/data/extract_{}.parquet'.format(run_date_str)) # Broadcast job end notification class End(Worker): def run(self): self.logger.info('Completed example batch process') run_date_str = self.context['run_date_str'] emit_slack_message('Successfully completed daily data download for {}'.format(run_date_str)) Task List File To specify the order of execution of Workers above, we need to express in a .lst (to be placed in <app_root_dir>/config/my_sample_app.lst ) file like: #PYTHON #ID|PARENT_IDS|MAX_ATTEMPTS|RETRY_WAIT_TIME|PROCESS_NAME |MODULE_NAME|WORKER_NAME |ARGUMENTS|LOGFILE 1 |-1 |1 |0 |Start Job Notification|workers |Start | |$ENV{APP_LOG_DIR}/start.log 2 |1 |1 |0 |Download File 1 |workers |DownloadFile1| |$ENV{APP_LOG_DIR}/download_1.log 3 |1 |1 |0 |Download File 2 |workers |DownloadFile2| |$ENV{APP_LOG_DIR}/download_2.log 4 |2 |1 |0 |Load MySQL Table |workers |LoadMySQL | |$ENV{APP_LOG_DIR}/load_mysql.log 5 |3 |1 |0 |Write Parquet File |workers |WriteParquet | |$ENV{APP_LOG_DIR}/write_parquet.log 6 |4,5 |1 |0 |End Job Notification |workers |End | |$ENV{APP_LOG_DIR}/end.log A visualization of the above .lst file: With the above two pieces, we now have the necessary code to run the original script as a PyRunner application.","title":"Example"},{"location":"example/#sample-python-script","text":"Let's take a very simple (and very much contrived) example: import urllib.request import pandas as pd from datetime import datetime def emit_slack_message(message): # Assume implementation # ... def load_to_mysql(df): # Assume implementation # ... def write_as_parquet(df, path): # Assume implementation # ... # Broadcast job start notification print('Starting example batch process') run_date_str = datetime.now().strftime('%Y-%m-%d') emit_slack_message('Starting daily data download for {}'.format(run_date_str)) # Download data file 1 print('Initiating File Download 1') url1 = 'https://some_file_url_here_1' urllib.request.urlretrieve(url1, '/landing/data1.csv') # Download data file 2 print('Initiating File Download 2') url2 = 'https://some_file_url_here_2' urllib.request.urlretrieve(url2, '/landing/data2.csv') # Load file 1 to MYSQL print('Loading MYSQL table') df1 = pd.read_csv('/landing/data1.csv') load_to_mysql(df1) # Write combined file as parquet print('Writing Parquet file') df2 = pd.read_csv('/landing/data2.csv') write_as_parquet(df2, '/data/extract_{}.parquet'.format(run_date_str)) # Broadcast job end notification print('Completed example batch process') emit_slack_message('Successfully completed daily data download for {}'.format(run_date_str)) As with any script, all steps execute in serial fashion and upon failure, a restart will run all steps again.","title":"Sample Python Script"},{"location":"example/#worker-classes","text":"We can instead convert this into a PyRunner application with minor changes by separating each logical step into Worker classes: # <app_root_dir>/python/workers.py import urllib.request import pandas as pd from datetime import datetime from pyrunner import Worker def emit_slack_message(message): # Assume implementation # ... def load_to_mysql(df): # Assume implementation # ... def write_as_parquet(df, path): # Assume implementation # ... # Broadcast job start notification class Start(Worker): def run(self): # Print function also works, but we can take advantage of # advanced features with the provided logger. self.logger.info('Starting example batch process') run_date_str = datetime.now().strftime('%Y-%m-%d') emit_slack_message('Starting daily data download for {}'.format(run_date_str)) # The self.context is a special thread-safe shared dictionary, # which can be read or modified from any Worker. self.context['run_date_str'] = run_date_str # Download data file 1 class DownloadFile1(Worker): def run(self): self.logger.info('Initiating File Download 1') url = 'https://some_file_url_here_1' urllib.request.urlretrieve(url, '/landing/data1.csv') # Download data file 2 class DownloadFile2(Worker): def run(self): self.logger.info('Initiating File Download 2') url = 'https://some_file_url_here_2' urllib.request.urlretrieve(url, '/landing/data2.csv') # Load file 1 to MYSQL class LoadMySQL(Worker): def run(self): self.logger.info('Loading MySQL table') df = pd.read_csv('/landing/data1.csv') load_to_mysql(df) # Write file 2 as parquet class WriteParquet(Worker): def run(self): self.logger.info('Writing Parquet file') # Once again, we are accessing the shared dictionary, this time # to read the value originally set by the Start Worker. run_date_str = self.context['run_date_str'] df = pd.read_csv('/landing/data2.csv') write_as_parquet(df, '/data/extract_{}.parquet'.format(run_date_str)) # Broadcast job end notification class End(Worker): def run(self): self.logger.info('Completed example batch process') run_date_str = self.context['run_date_str'] emit_slack_message('Successfully completed daily data download for {}'.format(run_date_str))","title":"Worker Classes"},{"location":"example/#task-list-file","text":"To specify the order of execution of Workers above, we need to express in a .lst (to be placed in <app_root_dir>/config/my_sample_app.lst ) file like: #PYTHON #ID|PARENT_IDS|MAX_ATTEMPTS|RETRY_WAIT_TIME|PROCESS_NAME |MODULE_NAME|WORKER_NAME |ARGUMENTS|LOGFILE 1 |-1 |1 |0 |Start Job Notification|workers |Start | |$ENV{APP_LOG_DIR}/start.log 2 |1 |1 |0 |Download File 1 |workers |DownloadFile1| |$ENV{APP_LOG_DIR}/download_1.log 3 |1 |1 |0 |Download File 2 |workers |DownloadFile2| |$ENV{APP_LOG_DIR}/download_2.log 4 |2 |1 |0 |Load MySQL Table |workers |LoadMySQL | |$ENV{APP_LOG_DIR}/load_mysql.log 5 |3 |1 |0 |Write Parquet File |workers |WriteParquet | |$ENV{APP_LOG_DIR}/write_parquet.log 6 |4,5 |1 |0 |End Job Notification |workers |End | |$ENV{APP_LOG_DIR}/end.log A visualization of the above .lst file: With the above two pieces, we now have the necessary code to run the original script as a PyRunner application.","title":"Task List File"},{"location":"design/","text":"Design Overview","title":"Overview"},{"location":"design/#design-overview","text":"","title":"Design Overview"},{"location":"design/config/","text":"Config Centralized configuration system. The Config class emulates container objects such that key value pairs can be accessed much like the way they are accessed in dictionaries. In [1]: from pyrunner import PyRunner In [2]: pr = PyRunner() In [3]: pr.config['app_name'] = 'MyNewApp' In [4]: pr.config['app_name'] Out[3]: 'MyNewApp' Config attributes are typed and additionally have multiple values that each may reference. The value assigned to a given attribute is taken from the first not-None source, checked in the following order: Manually assigned value Environment variable Hard-coded default In [1]: pr.config['app_name'] Out[1]: 'PyrunnerApp_5aa9a44c-a252-4505-89f9-b2e741bc1262' In [2]: os.environ['APP_NAME'] = 'MyNewApp' In [3]: pr.config['app_name'] Out[3]: 'MyNewApp' In [4]: pr.config['app_name'] = 'HelloWorldApp' Out[4]: 'HelloWorldApp' In [5]: del pr.config['app_name'] In [6]: pr.config['app_name'] Out[6]: 'MyNewApp'","title":"Config"},{"location":"design/config/#config","text":"Centralized configuration system. The Config class emulates container objects such that key value pairs can be accessed much like the way they are accessed in dictionaries. In [1]: from pyrunner import PyRunner In [2]: pr = PyRunner() In [3]: pr.config['app_name'] = 'MyNewApp' In [4]: pr.config['app_name'] Out[3]: 'MyNewApp' Config attributes are typed and additionally have multiple values that each may reference. The value assigned to a given attribute is taken from the first not-None source, checked in the following order: Manually assigned value Environment variable Hard-coded default In [1]: pr.config['app_name'] Out[1]: 'PyrunnerApp_5aa9a44c-a252-4505-89f9-b2e741bc1262' In [2]: os.environ['APP_NAME'] = 'MyNewApp' In [3]: pr.config['app_name'] Out[3]: 'MyNewApp' In [4]: pr.config['app_name'] = 'HelloWorldApp' Out[4]: 'HelloWorldApp' In [5]: del pr.config['app_name'] In [6]: pr.config['app_name'] Out[6]: 'MyNewApp'","title":"Config"},{"location":"user_guide/app_profile/","text":"First of two minimally required files if executing in Driverless mode . In Driver mode this file may be omitted, however, it is usually convenient and beneficial to have an explicit application profile. This file serves a similar purpose as the .profile/.bash_profile/.zprofile, but at the application instance level. The app_profile is sourced at the start of every job/batch application instance. In other words, it exports all required environment variables at the very start of execution. #!/bin/bash # This app_profile will be sourced prior to execution of PyRunner job. # NOTE: Only variables with \"APP_\" prefix will be available during job. # All other variables will be discarded. export APP_VERSION=0.0.1 export APP_NAME=\"sample_project\" export APP_ROOT_DIR=\"$(cd $(dirname ${BASH_SOURCE})/..; pwd)\" export APP_CONFIG_DIR=\"${APP_ROOT_DIR}/config\" export APP_TEMP_DIR=\"${APP_ROOT_DIR}/temp\" export APP_ROOT_LOG_DIR=\"${APP_ROOT_DIR}/logs\" export APP_LOG_RETENTION=\"30\" export APP_WORKER_DIR=\"${APP_ROOT_DIR}/workers\" DATE=$(date +\"%Y-%m-%d\") export APP_EXEC_TIMESTAMP=$(date +\"%Y%m%d_%H%M%S\") export APP_LOG_DIR=\"${APP_ROOT_LOG_DIR}/${DATE}\" if [ ! -e ${APP_LOG_DIR} ]; then mkdir -p ${APP_LOG_DIR}; fi if [ ! -e ${APP_TEMP_DIR} ]; then mkdir ${APP_TEMP_DIR}; fi Syntax is basic SHELL syntax. Note that only variables that are prefixed with 'APP_' will be accessible during execution after the app_profile is sourced. All other variables will be lost.","title":"Application Profile"},{"location":"user_guide/driver/","text":"Driver mode provides the option to implement a more dynamic batch process. While the app_profile and .lst files may still be used in this mode, they are no longer strictly required. In the custom driver program, the application's Config and ExecutionGraph must be manually constructed. Since the ExecutionGraph can now be constructed at runtime, it is no longer restricted to a static .lst file and the graph may be shaped based on logic implemented in the driver.","title":"Driver Mode"},{"location":"user_guide/driverless/","text":"Driverless mode is the simplest form of a PyRunner application and is suited for batch processes that follow one or few static workflows. This form is termed \"driverless\" as it does not require that a custom Python driver program to be written as it's entry point. Driverless mode requires only the app_profile and .lst file file: # Basic execution with only app_profile and .lst file pyrunner -c <app_root_path>/config/app_profile -l <app_root_path>/config/<project_name>.lst","title":"Driverless Mode"},{"location":"user_guide/lst_file/","text":"Process List File Second of two minimally required files if executing in Driverless mode . In Driver mode this file may be omitted, however, it is usually convenient and beneficial to have an explicit process list file. This is a serialization of the Execution graph (a DAG) and defines all tasks to be executed, as well as their dependencies on each other. Sample .lst File #PYTHON #ID|PARENT_IDS|MAX_ATTEMPTS|RETRY_WAIT_TIME|PROCESS_NAME|MODULE_NAME|WORKER_NAME|ARGUMENTS|LOGFILE 1|-1|1|0|Start Job|job|JobStart||$ENV{APP_LOG_DIR}/start_job.log 2|1|1|0|Download Delta File|process|DownloadFile||$ENV{APP_LOG_DIR}/download_delta_file.log 3|1|1|0|Read from Table|process|ReadTable||$ENV{APP_LOG_DIR}/read_table.log 4|2,3|1|0|Write Results File|process|WriteResults||$ENV{APP_LOG_DIR}/write_results.log 5|4|1|0|End Job|job|JobEnd||$ENV{APP_LOG_DIR}/end_job.log Each line contains a single task defined by the following parameters (pipe-separated): Task ID : must be unique and 1 or higher Parent ID's : comma separated list of numbers that describe which tasks must successfully execute before this task will trigger Maximum # of Attempts : 1 will mean no retry upon failure; 3 will mean the task will retry upon failure up to 2 times, until success or executed a maximum of 3 times Retry Wait Time : number of seconds between retry attempts on failure Task Name : unique task name for easier identification Task Module : Python file under the worker directory. e.g. for the process module, a <app_root>/worker/process.py is expected Task Worker : see Worker page for more details Task Arguments : optional comma-separated positional arguments Path to Log File : can use $ENV{APP_LOG_DIR} to write to log dir specified in the app_profile Any environment variables can be referenced using the $ENV{} syntax and will be substituted at the start of execution.","title":"Process List File"},{"location":"user_guide/lst_file/#process-list-file","text":"Second of two minimally required files if executing in Driverless mode . In Driver mode this file may be omitted, however, it is usually convenient and beneficial to have an explicit process list file. This is a serialization of the Execution graph (a DAG) and defines all tasks to be executed, as well as their dependencies on each other.","title":"Process List File"},{"location":"user_guide/lst_file/#sample-lst-file","text":"#PYTHON #ID|PARENT_IDS|MAX_ATTEMPTS|RETRY_WAIT_TIME|PROCESS_NAME|MODULE_NAME|WORKER_NAME|ARGUMENTS|LOGFILE 1|-1|1|0|Start Job|job|JobStart||$ENV{APP_LOG_DIR}/start_job.log 2|1|1|0|Download Delta File|process|DownloadFile||$ENV{APP_LOG_DIR}/download_delta_file.log 3|1|1|0|Read from Table|process|ReadTable||$ENV{APP_LOG_DIR}/read_table.log 4|2,3|1|0|Write Results File|process|WriteResults||$ENV{APP_LOG_DIR}/write_results.log 5|4|1|0|End Job|job|JobEnd||$ENV{APP_LOG_DIR}/end_job.log Each line contains a single task defined by the following parameters (pipe-separated): Task ID : must be unique and 1 or higher Parent ID's : comma separated list of numbers that describe which tasks must successfully execute before this task will trigger Maximum # of Attempts : 1 will mean no retry upon failure; 3 will mean the task will retry upon failure up to 2 times, until success or executed a maximum of 3 times Retry Wait Time : number of seconds between retry attempts on failure Task Name : unique task name for easier identification Task Module : Python file under the worker directory. e.g. for the process module, a <app_root>/worker/process.py is expected Task Worker : see Worker page for more details Task Arguments : optional comma-separated positional arguments Path to Log File : can use $ENV{APP_LOG_DIR} to write to log dir specified in the app_profile Any environment variables can be referenced using the $ENV{} syntax and will be substituted at the start of execution.","title":"Sample .lst File"},{"location":"user_guide/project_structure/","text":"Directory Structure As described in the Quick Start, the directory structure and minimally required files can be automatically generated via ( logs and temp generated at first run): pyrunner --setup This will create the following project structure: config Directory The default directory in which the app_profile and .lst file files are created. This path can be referenced by env var ${APP_CONFIG_DIR} . logs Directory The root log directory referenced by env var ${APP_ROOT_LOG_DIR} . By default, logs will be written to a sub-directory named as the current date: ${APP_ROOT_LOG_DIR}/YYYY-MM-DD This date-stamped sub-directory path can be referenced by env var ${APP_LOG_DIR} . worker Directory Default location to store PyRunner Worker class implementations. All workers given in the .lst file (or driver program) are expected to be found in this directory. This path can be referenced by env var ${APP_WORKER_DIR} . temp Directory Default location for intermediate or checkpoint files generated by PyRunner. This path can be referenced by env var ${APP_TEMP_DIR} .","title":"Basic Project Structure"},{"location":"user_guide/project_structure/#directory-structure","text":"As described in the Quick Start, the directory structure and minimally required files can be automatically generated via ( logs and temp generated at first run): pyrunner --setup This will create the following project structure:","title":"Directory Structure"},{"location":"user_guide/project_structure/#config-directory","text":"The default directory in which the app_profile and .lst file files are created. This path can be referenced by env var ${APP_CONFIG_DIR} .","title":"config Directory"},{"location":"user_guide/project_structure/#logs-directory","text":"The root log directory referenced by env var ${APP_ROOT_LOG_DIR} . By default, logs will be written to a sub-directory named as the current date: ${APP_ROOT_LOG_DIR}/YYYY-MM-DD This date-stamped sub-directory path can be referenced by env var ${APP_LOG_DIR} .","title":"logs Directory"},{"location":"user_guide/project_structure/#worker-directory","text":"Default location to store PyRunner Worker class implementations. All workers given in the .lst file (or driver program) are expected to be found in this directory. This path can be referenced by env var ${APP_WORKER_DIR} .","title":"worker Directory"},{"location":"user_guide/project_structure/#temp-directory","text":"Default location for intermediate or checkpoint files generated by PyRunner. This path can be referenced by env var ${APP_TEMP_DIR} .","title":"temp Directory"},{"location":"user_guide/worker/","text":"Each task described in the process list file must be assigned a single Worker class. In the following line from an example process list: 2|1|1|0|Download Delta File|process|DownloadFile||$ENV{APP_LOG_DIR}/download_delta_file.log PyRunner will look within the ${APP_WORKER_DIR} (defined in the app_profile ) for class DownloadFile in process.py . The DownloadFile class is expected to extend the pyrunner.Worker abstract class and implement the run(self) method at minimum. Worker Properties Implementations of the Worker class have the following class properties available to them: self.argv - argument vector to access positional arguments optionally provided in the .lst file. self.logger - simple logger object with .info(<message>) and .error(<message>) methods that write provided string to the text file indicated in the .lst file ( $ENV{APP_LOG_DIR} in the above example). self.context - a thread-safe key/value store shared across all tasks within a given instance of a job, which provides the ability to share data across separate tasks. Worker Lifecycle Methods Additionally, there exist lifecycle methods (in addition to the mandatory run(self) method) that may optionally be implemented: on_success(self) - only invoked upon successful execution of the run(self) method. on_fail(self) - only invoked upon failed execution of the run(self) method. on_exit(self) - always invoked upon completion of the run(self) method. Return Codes and Exceptions Return values are not explicitly required for any lifecycle method - the absence of an explicit return or prematurely returning None/0 will result in ending the task in success. In the case that the task must be prematurely ended in failure, either a non-zero/not-None value may be returned: # Do something if my_value == 'something unexpected': self.logger.error('my_value is not assigned the appropriate value!') return 99 or an exception may be raised: # Do something if my_value == 'something unexpected': raise ValueError('my_value is not assigned the appropriate value!') Raising an exception is the preferred method of ending a task in failure. All exceptions, whether intentionally thrown or unexpectedly encountered, will have the exception message and stack trace written to the log file and return a non-zero return code.","title":"PyRunner Workers"},{"location":"user_guide/worker/#worker-properties","text":"Implementations of the Worker class have the following class properties available to them: self.argv - argument vector to access positional arguments optionally provided in the .lst file. self.logger - simple logger object with .info(<message>) and .error(<message>) methods that write provided string to the text file indicated in the .lst file ( $ENV{APP_LOG_DIR} in the above example). self.context - a thread-safe key/value store shared across all tasks within a given instance of a job, which provides the ability to share data across separate tasks.","title":"Worker Properties"},{"location":"user_guide/worker/#worker-lifecycle-methods","text":"Additionally, there exist lifecycle methods (in addition to the mandatory run(self) method) that may optionally be implemented: on_success(self) - only invoked upon successful execution of the run(self) method. on_fail(self) - only invoked upon failed execution of the run(self) method. on_exit(self) - always invoked upon completion of the run(self) method.","title":"Worker Lifecycle Methods"},{"location":"user_guide/worker/#return-codes-and-exceptions","text":"Return values are not explicitly required for any lifecycle method - the absence of an explicit return or prematurely returning None/0 will result in ending the task in success. In the case that the task must be prematurely ended in failure, either a non-zero/not-None value may be returned: # Do something if my_value == 'something unexpected': self.logger.error('my_value is not assigned the appropriate value!') return 99 or an exception may be raised: # Do something if my_value == 'something unexpected': raise ValueError('my_value is not assigned the appropriate value!') Raising an exception is the preferred method of ending a task in failure. All exceptions, whether intentionally thrown or unexpectedly encountered, will have the exception message and stack trace written to the log file and return a non-zero return code.","title":"Return Codes and Exceptions"}]}